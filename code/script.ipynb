{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test f(s,a,s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from DeepAIRL import DeepAIRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = np.load('static/fm.npy')\n",
    "expert_traj = {\n",
    "    'state':[],\n",
    "    'action':[],\n",
    "    'reward':[],\n",
    "    'next_state':[],\n",
    "    'done':[],\n",
    "    'labels':[]\n",
    "}\n",
    "expert_traj['state'] = np.random.rand(5, 12).tolist()\n",
    "action = np.random.randint(0, 3, 5).reshape(5, 1)\n",
    "expert_traj['encoded_action'] = tf.keras.utils.to_categorical(action, num_classes=3)\n",
    "expert_traj['action'] = action.tolist()\n",
    "expert_traj['reward'] = np.random.rand(5)\n",
    "expert_traj['next_state'] = np.random.rand(5, 12).tolist()\n",
    "expert_traj['done'] = np.zeros(5)\n",
    "expert_traj['labels'] = np.ones(5)\n",
    "traj_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gen_traj = {\n",
    "    'state':[],\n",
    "    'action':[],\n",
    "    'reward':[],\n",
    "    'next_state':[],\n",
    "    'done':[],\n",
    "    'label':[]\n",
    "}\n",
    "gen_traj['state'] = expert_traj['state']\n",
    "gen_traj['next_state'] = np.flipud(fm[0:10])\n",
    "gen_traj['done'] = np.zeros(10)\n",
    "gen_traj['action'] = np.random.randint(0, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = DeepAIRL(\n",
    "    env_flag='PF',\n",
    "    user_num=3,\n",
    "    rbg_num=1,\n",
    "    feature_dim=4,\n",
    "    n_states=1296,\n",
    "    n_actions=3,\n",
    "    traj=expert_traj,\n",
    "    traj_length=traj_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21906003, 0.4625738 , 0.31836614],\n",
       "       [0.20018406, 0.45232546, 0.34749043],\n",
       "       [0.17363249, 0.47224858, 0.35411894],\n",
       "       [0.214531  , 0.4788761 , 0.30659288],\n",
       "       [0.16707443, 0.48981553, 0.34311008]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = alg.DDPGAgent.actor_eval_net.predict(expert_traj['state'])\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4625738 , 0.45232546, 0.47224858, 0.4788761 , 0.48981553],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(policy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_pairs = np.concatenate((expert_traj['state'], expert_traj['encoded_action']), axis=1)\n",
    "sa_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    alg.reward = alg.reward_model(sa_pairs.astype('float32'))\n",
    "    shaping_term_s = alg.shaping_term(expert_traj['state'])\n",
    "    shaping_term_ns = alg.shaping_term(expert_traj['next_state'])\n",
    "    f_sas = alg.reward + alg.gamma * shaping_term_ns - shaping_term_s\n",
    "\n",
    "    part_policy = get_part_policy()\n",
    "    \n",
    "    D_sas = tf.exp(f_sas) / (tf.exp(f_sas) + part_policy)\n",
    "    \n",
    "    alg.loss = tf.keras.losses.binary_crossentropy(y_true=expert_traj['labels'], y_pred=D_sas)\n",
    "\n",
    "    gradients = tape.gradient(alg.loss, alg.reward_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, alg.reward_model.trainable_variables))\n",
    "    \n",
    "    del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test f(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from DeepAIRL import DeepAIRL\n",
    "from simulator.ProportionalFairness import load_av_ue_info, PROPORTIONALFAIRNESS\n",
    "\n",
    "def user_info_threshold(user_info, threshold_min, threshold_max):\n",
    "    filtered_user_info = []\n",
    "    for i in range(len(user_info)):\n",
    "        if (user_info[i]['buffer'] > threshold_min) and (user_info[i]['buffer'] < threshold_max):\n",
    "            filtered_user_info.append(user_info[i])\n",
    "    return filtered_user_info\n",
    "\n",
    "class Replayer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = pd.DataFrame(index=range(capacity),\n",
    "                                   columns=['state', 'action', 'reward', 'label'])\n",
    "        self.i = 0\n",
    "        self.count = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def store(self, *args):\n",
    "        self.memory.loc[self.i] = args\n",
    "        self.i = (self.i + 1) % self.capacity\n",
    "        self.count = min(self.count + 1, self.capacity)\n",
    "\n",
    "    def sample(self, size):\n",
    "        self.memory.dropna(inplace=True)\n",
    "        indices = np.random.choice(self.count, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\n",
    "                self.memory.columns)\n",
    "av_ues_info = load_av_ue_info()\n",
    "av_ues_info = user_info_threshold(av_ues_info, threshold_min=1e+6, threshold_max=1e+7)\n",
    "pf_env = PROPORTIONALFAIRNESS(\n",
    "    lambda_avg=None,\n",
    "    lambda_fairness=None,\n",
    "    reward_flag=None)\n",
    "\n",
    "INITIAL_USER_START = 0\n",
    "USER_NUM = 3\n",
    "av_ues_idx = list(range(INITIAL_USER_START, INITIAL_USER_START + USER_NUM))\n",
    "state = pf_env.reset(av_ues_info, av_ues_idx)\n",
    "replayer = Replayer(capacity=500)\n",
    "\n",
    "tti = 0 \n",
    "while (tti < 500):\n",
    "    action = pf_env.action()[0]\n",
    "    next_state, reward, done, info = pf_env.step(None, 0, 0)\n",
    "    replayer.store(state.reshape(-1), action, reward, 1.0)\n",
    "    state = next_state\n",
    "    tti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replayer.memory.dropna(inplace=True)\n",
    "replayer.memory.reset_index(inplace=True, drop=True)\n",
    "alg = DeepAIRL(\n",
    "    env_flag='PF',\n",
    "    user_num=3,\n",
    "    rbg_num=1,\n",
    "    feature_dim=4,\n",
    "    n_states=1296,\n",
    "    n_actions=3,\n",
    "    traj=replayer.memory,\n",
    "    traj_length=replayer.memory.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.execute_policy()\n",
    "alg.traj_batch = alg.get_batch()\n",
    "encoded_actions = tf.keras.utils.to_categorical(alg.traj_batch['action'], num_classes=alg.user_num)\n",
    "sa_pairs = np.concatenate((alg.traj_batch['state'], encoded_actions), axis=1)\n",
    "alg.D_output = alg.discriminator.predict(sa_pairs)\n",
    "alg.reward = alg.update_reward()\n",
    "alg.loss = alg.logistic_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Generating trajectories!\n",
      "INFO: Trajectories generation accomplished, time cost=0.5716619999999999s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from DeepAIRL import DeepAIRL\n",
    "from collect import COLLECT\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.disable(30)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    USER_NUM = 3\n",
    "    RBG_NUM = 1\n",
    "    FEATURE_DIM = 4\n",
    "    N_STATES = None\n",
    "    N_ACTIONS = USER_NUM\n",
    "    TRAJ_NUMBER = 5\n",
    "    TRAJ_LENGTH = 50\n",
    "    EPOCH = 100\n",
    "    ENV_FLAG = ['PF']\n",
    "\n",
    "    for flag in ENV_FLAG:\n",
    "        collect = COLLECT(\n",
    "            env_flag=flag,\n",
    "            user_num=USER_NUM,\n",
    "            feature_dim=FEATURE_DIM,\n",
    "            rbg_num=RBG_NUM,\n",
    "            traj_length=TRAJ_LENGTH,\n",
    "            traj_number=TRAJ_NUMBER,\n",
    "            replayer_capacity=TRAJ_LENGTH * TRAJ_NUMBER\n",
    "        )\n",
    "\n",
    "        expert_traj = collect.generate()\n",
    "        \n",
    "        alg = DeepAIRL(\n",
    "            env_flag=flag,\n",
    "            user_num=USER_NUM,\n",
    "            rbg_num=RBG_NUM,\n",
    "            epochs=EPOCH,\n",
    "            feature_dim=FEATURE_DIM,\n",
    "            n_states=N_STATES,\n",
    "            n_actions=N_ACTIONS,\n",
    "            traj=expert_traj,\n",
    "            traj_length=expert_traj.shape[0],\n",
    "            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
